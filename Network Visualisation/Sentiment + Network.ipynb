{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/behemoth/anaconda3/lib/python3.11/site-packages/dash_bootstrap_components/_table.py:5: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm \n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from jaal import Jaal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEO reccomendation and trending topics over time \n",
    "\n",
    "We scraped the articles using beautiful soup to create a SEO reccomender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ncourierdic = scrape_article_data(df_courier)\\n\\n\\npreesanddic = scrape_article_data(df_pressandjournal)\\nrspvdic = scrape_article_data(df_rsvplive)\\nhufftingdic = scrape_article_data(df_huffington)\\nlicolndic = scrape_article_data(df_lincolnshire)\\ndf_stylist_dictionary = scrape_article_data(df_stylist)\\ndf_ok_dictionary = scrape_article_data(df_ok)\\ndf_corkbeo_dictionary = scrape_article_data(df_corkbeo)\\nCityam_dictionary = scrape_article_data(df_cityam)\\nDublin_dictionary = scrape_article_data(df_dublinlive)\\nHertfordshire_dictionary = scrape_article_data(df_hertfordshire)\\nEmpire_dictionary = scrape_article_data(df_empireonline)\\nSussex_dictionary = scrape_article_data(df_sussexlive)\\nHampshire_dictionary = scrape_article_data(df_hampshirelive)\\nand converted them to json\\n\\nwith open('ok.json', 'w') as file:\\n    json.dump(df_ok_dictionary, file)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have gathered some basic info let us scrape the data from the article now \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_article_data(df):\n",
    "    scraped_articles = []\n",
    "    for article_url in df['page_url']:\n",
    "        time.sleep(random.uniform(0.5, 5.0))  \n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  \n",
    "        response = requests.get(article_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = soup.find('h1').text.strip() if soup.find('h1') else 'Unknown'\n",
    "            article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "            authors = soup.find('span', class_='author-name').text.strip() if soup.find('span', class_='author-name') else 'Unknown'\n",
    "            publish_date = soup.find('time').text.strip() if soup.find('time') else 'Unknown'\n",
    "            scraped_data = {\n",
    "                'url': article_url,\n",
    "                'title': title,\n",
    "                'text': article_text,\n",
    "                'authors': authors,\n",
    "                'publish_date': publish_date\n",
    "            }\n",
    "            scraped_articles.append(scraped_data)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the article from {article_url}. Status code: {response.status_code}\")\n",
    "    return scraped_articles\n",
    "\n",
    "#scraped articles for top 15 news sources \n",
    "\n",
    "'''\n",
    "\n",
    "courierdic = scrape_article_data(df_courier)\n",
    "\n",
    "\n",
    "preesanddic = scrape_article_data(df_pressandjournal)\n",
    "rspvdic = scrape_article_data(df_rsvplive)\n",
    "hufftingdic = scrape_article_data(df_huffington)\n",
    "licolndic = scrape_article_data(df_lincolnshire)\n",
    "df_stylist_dictionary = scrape_article_data(df_stylist)\n",
    "df_ok_dictionary = scrape_article_data(df_ok)\n",
    "df_corkbeo_dictionary = scrape_article_data(df_corkbeo)\n",
    "Cityam_dictionary = scrape_article_data(df_cityam)\n",
    "Dublin_dictionary = scrape_article_data(df_dublinlive)\n",
    "Hertfordshire_dictionary = scrape_article_data(df_hertfordshire)\n",
    "Empire_dictionary = scrape_article_data(df_empireonline)\n",
    "Sussex_dictionary = scrape_article_data(df_sussexlive)\n",
    "Hampshire_dictionary = scrape_article_data(df_hampshirelive)\n",
    "and converted them to json\n",
    "\n",
    "with open('ok.json', 'w') as file:\n",
    "    json.dump(df_ok_dictionary, file)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New df with sentiment analysis included with VADER\n",
    "Created a bigger dataframe from the single dictionaries and added a newsource column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>wikiLink</th>\n",
       "      <th>score</th>\n",
       "      <th>wikidataId</th>\n",
       "      <th>page_url</th>\n",
       "      <th>clean_url</th>\n",
       "      <th>processed_url</th>\n",
       "      <th>news_source</th>\n",
       "      <th>page_type</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>authors</th>\n",
       "      <th>publish_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Rail transport</td>\n",
       "      <td>http://en.wikipedia.org/Category:Rail_transport</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>Q3565868</td>\n",
       "      <td>https://www.dublinlive.ie/news/dublin-traffic-...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie</td>\n",
       "      <td>news</td>\n",
       "      <td>Dublin traffic LIVE: Crash near Lucan, Irish R...</td>\n",
       "      <td>We will bring you all the latest traffic updat...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>Sustainable transport</td>\n",
       "      <td>http://en.wikipedia.org/Category:Sustainable_t...</td>\n",
       "      <td>0.92090</td>\n",
       "      <td>Q1364310</td>\n",
       "      <td>https://www.dublinlive.ie/news/dublin-traffic-...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie</td>\n",
       "      <td>news</td>\n",
       "      <td>Dublin traffic LIVE: Crash near Lucan, Irish R...</td>\n",
       "      <td>We will bring you all the latest traffic updat...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>M7 motorway (Ireland)</td>\n",
       "      <td>http://en.wikipedia.org/M7_motorway_(Ireland)</td>\n",
       "      <td>0.65780</td>\n",
       "      <td>Q3775884</td>\n",
       "      <td>https://www.dublinlive.ie/news/dublin-traffic-...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie</td>\n",
       "      <td>news</td>\n",
       "      <td>Dublin traffic LIVE: Crash near Lucan, Irish R...</td>\n",
       "      <td>We will bring you all the latest traffic updat...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Vehicles</td>\n",
       "      <td>http://en.wikipedia.org/Category:Vehicles</td>\n",
       "      <td>0.56240</td>\n",
       "      <td>Q42889</td>\n",
       "      <td>https://www.dublinlive.ie/news/dublin-traffic-...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie</td>\n",
       "      <td>news</td>\n",
       "      <td>Dublin traffic LIVE: Crash near Lucan, Irish R...</td>\n",
       "      <td>We will bring you all the latest traffic updat...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>Passenger rail transport</td>\n",
       "      <td>http://en.wikipedia.org/Category:Passenger_rai...</td>\n",
       "      <td>0.50640</td>\n",
       "      <td>Q1433891</td>\n",
       "      <td>https://www.dublinlive.ie/news/dublin-traffic-...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie/news/dublin-traffic-live-crash-n...</td>\n",
       "      <td>dublinlive.ie</td>\n",
       "      <td>news</td>\n",
       "      <td>Dublin traffic LIVE: Crash near Lucan, Irish R...</td>\n",
       "      <td>We will bring you all the latest traffic updat...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271912</th>\n",
       "      <td>100</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>http://en.wikipedia.org/Category:Cancer</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>Q12078</td>\n",
       "      <td>https://www.cheshire-live.co.uk/news/chester-c...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk</td>\n",
       "      <td>news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271913</th>\n",
       "      <td>110</td>\n",
       "      <td>Coronavirus-associated diseases</td>\n",
       "      <td>http://en.wikipedia.org/Category:Coronavirus-a...</td>\n",
       "      <td>0.13780</td>\n",
       "      <td>Q107361528</td>\n",
       "      <td>https://www.cheshire-live.co.uk/news/chester-c...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk</td>\n",
       "      <td>news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271914</th>\n",
       "      <td>116</td>\n",
       "      <td>Fascia</td>\n",
       "      <td>http://en.wikipedia.org/Category:Fascia</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>Q936531</td>\n",
       "      <td>https://www.cheshire-live.co.uk/news/chester-c...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk</td>\n",
       "      <td>news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271915</th>\n",
       "      <td>136</td>\n",
       "      <td>Coronaviruses</td>\n",
       "      <td>http://en.wikipedia.org/Category:Coronaviruses</td>\n",
       "      <td>0.09304</td>\n",
       "      <td>Q290805</td>\n",
       "      <td>https://www.cheshire-live.co.uk/news/chester-c...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk</td>\n",
       "      <td>news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271916</th>\n",
       "      <td>161</td>\n",
       "      <td>Virology</td>\n",
       "      <td>http://en.wikipedia.org/Category:Virology</td>\n",
       "      <td>0.05243</td>\n",
       "      <td>Q7215</td>\n",
       "      <td>https://www.cheshire-live.co.uk/news/chester-c...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk/news/chester-cheshire-news...</td>\n",
       "      <td>cheshire-live.co.uk</td>\n",
       "      <td>news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1271917 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                            label  \\\n",
       "0          7                   Rail transport   \n",
       "1         15            Sustainable transport   \n",
       "2         21            M7 motorway (Ireland)   \n",
       "3         28                         Vehicles   \n",
       "4         38         Passenger rail transport   \n",
       "...      ...                              ...   \n",
       "1271912  100                           Cancer   \n",
       "1271913  110  Coronavirus-associated diseases   \n",
       "1271914  116                           Fascia   \n",
       "1271915  136                    Coronaviruses   \n",
       "1271916  161                         Virology   \n",
       "\n",
       "                                                  wikiLink    score  \\\n",
       "0          http://en.wikipedia.org/Category:Rail_transport  1.00000   \n",
       "1        http://en.wikipedia.org/Category:Sustainable_t...  0.92090   \n",
       "2            http://en.wikipedia.org/M7_motorway_(Ireland)  0.65780   \n",
       "3                http://en.wikipedia.org/Category:Vehicles  0.56240   \n",
       "4        http://en.wikipedia.org/Category:Passenger_rai...  0.50640   \n",
       "...                                                    ...      ...   \n",
       "1271912            http://en.wikipedia.org/Category:Cancer  0.16090   \n",
       "1271913  http://en.wikipedia.org/Category:Coronavirus-a...  0.13780   \n",
       "1271914            http://en.wikipedia.org/Category:Fascia  0.12180   \n",
       "1271915     http://en.wikipedia.org/Category:Coronaviruses  0.09304   \n",
       "1271916          http://en.wikipedia.org/Category:Virology  0.05243   \n",
       "\n",
       "         wikidataId                                           page_url  \\\n",
       "0          Q3565868  https://www.dublinlive.ie/news/dublin-traffic-...   \n",
       "1          Q1364310  https://www.dublinlive.ie/news/dublin-traffic-...   \n",
       "2          Q3775884  https://www.dublinlive.ie/news/dublin-traffic-...   \n",
       "3            Q42889  https://www.dublinlive.ie/news/dublin-traffic-...   \n",
       "4          Q1433891  https://www.dublinlive.ie/news/dublin-traffic-...   \n",
       "...             ...                                                ...   \n",
       "1271912      Q12078  https://www.cheshire-live.co.uk/news/chester-c...   \n",
       "1271913  Q107361528  https://www.cheshire-live.co.uk/news/chester-c...   \n",
       "1271914     Q936531  https://www.cheshire-live.co.uk/news/chester-c...   \n",
       "1271915     Q290805  https://www.cheshire-live.co.uk/news/chester-c...   \n",
       "1271916       Q7215  https://www.cheshire-live.co.uk/news/chester-c...   \n",
       "\n",
       "                                                 clean_url  \\\n",
       "0        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "1        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "2        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "3        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "4        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "...                                                    ...   \n",
       "1271912  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271913  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271914  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271915  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271916  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "\n",
       "                                             processed_url  \\\n",
       "0        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "1        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "2        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "3        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "4        dublinlive.ie/news/dublin-traffic-live-crash-n...   \n",
       "...                                                    ...   \n",
       "1271912  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271913  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271914  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271915  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "1271916  cheshire-live.co.uk/news/chester-cheshire-news...   \n",
       "\n",
       "                 news_source page_type  \\\n",
       "0              dublinlive.ie      news   \n",
       "1              dublinlive.ie      news   \n",
       "2              dublinlive.ie      news   \n",
       "3              dublinlive.ie      news   \n",
       "4              dublinlive.ie      news   \n",
       "...                      ...       ...   \n",
       "1271912  cheshire-live.co.uk      news   \n",
       "1271913  cheshire-live.co.uk      news   \n",
       "1271914  cheshire-live.co.uk      news   \n",
       "1271915  cheshire-live.co.uk      news   \n",
       "1271916  cheshire-live.co.uk      news   \n",
       "\n",
       "                                                     title  \\\n",
       "0        Dublin traffic LIVE: Crash near Lucan, Irish R...   \n",
       "1        Dublin traffic LIVE: Crash near Lucan, Irish R...   \n",
       "2        Dublin traffic LIVE: Crash near Lucan, Irish R...   \n",
       "3        Dublin traffic LIVE: Crash near Lucan, Irish R...   \n",
       "4        Dublin traffic LIVE: Crash near Lucan, Irish R...   \n",
       "...                                                    ...   \n",
       "1271912                                                NaN   \n",
       "1271913                                                NaN   \n",
       "1271914                                                NaN   \n",
       "1271915                                                NaN   \n",
       "1271916                                                NaN   \n",
       "\n",
       "                                                      text  authors  \\\n",
       "0        We will bring you all the latest traffic updat...  Unknown   \n",
       "1        We will bring you all the latest traffic updat...  Unknown   \n",
       "2        We will bring you all the latest traffic updat...  Unknown   \n",
       "3        We will bring you all the latest traffic updat...  Unknown   \n",
       "4        We will bring you all the latest traffic updat...  Unknown   \n",
       "...                                                    ...      ...   \n",
       "1271912                                                NaN      NaN   \n",
       "1271913                                                NaN      NaN   \n",
       "1271914                                                NaN      NaN   \n",
       "1271915                                                NaN      NaN   \n",
       "1271916                                                NaN      NaN   \n",
       "\n",
       "        publish_date  \n",
       "0                     \n",
       "1                     \n",
       "2                     \n",
       "3                     \n",
       "4                     \n",
       "...              ...  \n",
       "1271912          NaN  \n",
       "1271913          NaN  \n",
       "1271914          NaN  \n",
       "1271915          NaN  \n",
       "1271916          NaN  \n",
       "\n",
       "[1271917 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import json\n",
    "\n",
    "json_files = [file for file in os.listdir('.') if file.endswith('.json')]\n",
    "\n",
    "dfs = []\n",
    "for file in json_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        df_to_append = pd.DataFrame(data)\n",
    "        dfs.append(df_to_append)\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df.rename(columns={'url': 'page_url'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Join the merged dataframe with the original dataframe based on the 'url' column\n",
    "scraped_df = pd.merge(df, merged_df, on='page_url', how='left')\n",
    "scraped_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>authors</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.corkbeo.ie/news/another-terrible-r...</td>\n",
       "      <td>Another terrible road tragedy as 3-year-old gi...</td>\n",
       "      <td>Five people have died on Irish roads in the la...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "      <td>-0.9963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.corkbeo.ie/news/derry-girls-siobhn...</td>\n",
       "      <td>Derry Girls' Siobhán McSweeney raises thousand...</td>\n",
       "      <td>The star announced the winner of her stunning ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "      <td>0.9944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.corkbeo.ie/news/local-news/young-b...</td>\n",
       "      <td>Young boy who passed away in Cork Harbour trag...</td>\n",
       "      <td>The 14-year-old had been out swimming with fri...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "      <td>0.8735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.corkbeo.ie/news/local-news/four-yo...</td>\n",
       "      <td>Four young people killed in Co Tipperary car c...</td>\n",
       "      <td>A man in his early 20s and three female teenag...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "      <td>-0.9654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.corkbeo.ie/news/local-news/one-man...</td>\n",
       "      <td>One man arrested as Gardaí raid property in No...</td>\n",
       "      <td>Gardaí seized a huge haul of suspected cannabi...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "      <td>-0.6553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>https://cityam.com/new-clinical-trial-hub-to-o...</td>\n",
       "      <td>New clinical trial hub to open in Canary Wharf...</td>\n",
       "      <td>Featured Submit a story Tell us your story. Fe...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Tuesday 29 August 2023 7:42 am</td>\n",
       "      <td>0.9528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>https://cityam.com/even-on-their-favourite-bat...</td>\n",
       "      <td>Even on their favourite battlegrounds, the Con...</td>\n",
       "      <td>Featured Submit a story Tell us your story. Fe...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Thursday 31 August 2023 5:30 am</td>\n",
       "      <td>-0.9926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>https://cityam.com/ftse-100-live-london-flat-o...</td>\n",
       "      <td>FTSE 100 Close: London finishes in red on fear...</td>\n",
       "      <td>Featured Submit a story Tell us your story. Fe...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Thursday 31 August 2023 10:17 am</td>\n",
       "      <td>0.9890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>https://cityam.com/glencores-shares-slide-as-1...</td>\n",
       "      <td>Glencore’s shares slide as 197 claimants for l...</td>\n",
       "      <td>Featured Submit a story Tell us your story. Fe...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Thursday 31 August 2023 11:46 am</td>\n",
       "      <td>-0.8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>https://cityam.com/who-are-frontrunners-in-the...</td>\n",
       "      <td>Who are the frontrunners in the race to save W...</td>\n",
       "      <td>Featured Submit a story Tell us your story. Fe...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Wednesday 30 August 2023 3:10 pm</td>\n",
       "      <td>0.9904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3402 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               page_url  \\\n",
       "0     https://www.corkbeo.ie/news/another-terrible-r...   \n",
       "1     https://www.corkbeo.ie/news/derry-girls-siobhn...   \n",
       "2     https://www.corkbeo.ie/news/local-news/young-b...   \n",
       "3     https://www.corkbeo.ie/news/local-news/four-yo...   \n",
       "4     https://www.corkbeo.ie/news/local-news/one-man...   \n",
       "...                                                 ...   \n",
       "3397  https://cityam.com/new-clinical-trial-hub-to-o...   \n",
       "3398  https://cityam.com/even-on-their-favourite-bat...   \n",
       "3399  https://cityam.com/ftse-100-live-london-flat-o...   \n",
       "3400  https://cityam.com/glencores-shares-slide-as-1...   \n",
       "3401  https://cityam.com/who-are-frontrunners-in-the...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Another terrible road tragedy as 3-year-old gi...   \n",
       "1     Derry Girls' Siobhán McSweeney raises thousand...   \n",
       "2     Young boy who passed away in Cork Harbour trag...   \n",
       "3     Four young people killed in Co Tipperary car c...   \n",
       "4     One man arrested as Gardaí raid property in No...   \n",
       "...                                                 ...   \n",
       "3397  New clinical trial hub to open in Canary Wharf...   \n",
       "3398  Even on their favourite battlegrounds, the Con...   \n",
       "3399  FTSE 100 Close: London finishes in red on fear...   \n",
       "3400  Glencore’s shares slide as 197 claimants for l...   \n",
       "3401  Who are the frontrunners in the race to save W...   \n",
       "\n",
       "                                                   text  authors  \\\n",
       "0     Five people have died on Irish roads in the la...  Unknown   \n",
       "1     The star announced the winner of her stunning ...  Unknown   \n",
       "2     The 14-year-old had been out swimming with fri...  Unknown   \n",
       "3     A man in his early 20s and three female teenag...  Unknown   \n",
       "4     Gardaí seized a huge haul of suspected cannabi...  Unknown   \n",
       "...                                                 ...      ...   \n",
       "3397  Featured Submit a story Tell us your story. Fe...  Unknown   \n",
       "3398  Featured Submit a story Tell us your story. Fe...  Unknown   \n",
       "3399  Featured Submit a story Tell us your story. Fe...  Unknown   \n",
       "3400  Featured Submit a story Tell us your story. Fe...  Unknown   \n",
       "3401  Featured Submit a story Tell us your story. Fe...  Unknown   \n",
       "\n",
       "                          publish_date  sentiment  \n",
       "0                                         -0.9963  \n",
       "1                                          0.9944  \n",
       "2                                          0.8735  \n",
       "3                                         -0.9654  \n",
       "4                                         -0.6553  \n",
       "...                                ...        ...  \n",
       "3397    Tuesday 29 August 2023 7:42 am     0.9528  \n",
       "3398   Thursday 31 August 2023 5:30 am    -0.9926  \n",
       "3399  Thursday 31 August 2023 10:17 am     0.9890  \n",
       "3400  Thursday 31 August 2023 11:46 am    -0.8360  \n",
       "3401  Wednesday 30 August 2023 3:10 pm     0.9904  \n",
       "\n",
       "[3402 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment\n",
    "def get_sentiment(text):\n",
    "    return sia.polarity_scores(text)['compound']  # Returns the compound score\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "merged_df['sentiment'] = merged_df['text'].apply(get_sentiment)\n",
    "\n",
    "df_keywords = merged_df.drop_duplicates(subset=['page_url'])\n",
    "df_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_core_web_sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m en_core_web_sm\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to extract keywords using Spacy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_keywords\u001b[39m(text):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'en_core_web_sm' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Function to extract keywords using Spacy\n",
    "def extract_keywords(text):\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        keywords = [(ent.text, ent.label_) for ent in doc.ents][:10]  # Extract the first 10 entities\n",
    "    except NameError:\n",
    "        return []\n",
    "    return keywords\n",
    "\n",
    "# Apply the function to extract keywords from df_keywords\n",
    "df_keywords['keywords'] = df_keywords['text'].apply(lambda x: extract_keywords(x))\n",
    "\n",
    "# Display the keywords for the first few articles\n",
    "print(df_keywords[['text', 'keywords']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FAC', 'LOC', 'TIME', 'WORK_OF_ART', 'QUANTITY', 'LANGUAGE', 'EVENT', 'PERCENT', 'ORG', 'NORP', 'PRODUCT', 'PERSON', 'DATE', 'GPE', 'CARDINAL', 'LAW', 'MONEY', 'ORDINAL'}\n"
     ]
    }
   ],
   "source": [
    "df_keywords.to_csv('df_withkeywords.csv')\n",
    "# Extract unique values from the 'keywords' column and print them as a list of lists\n",
    "unique_keywords = df_keywords['keywords'].explode().unique()\n",
    "unique_keyword_types = [keyword[1] for keyword in unique_keywords if isinstance(keyword, tuple)]\n",
    "print(set(unique_keyword_types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "def standardize_date(date_str):\n",
    "    if pd.isnull(date_str):  # Check if date_str is null\n",
    "        return None\n",
    "\n",
    "    standardized_date = pd.NaT  # Defaulting to 'Not a Time' for error handling\n",
    "\n",
    "    # Preprocess the date string to handle different formats\n",
    "    if isinstance(date_str, str):  # Check if date_str is a string\n",
    "        date_str = date_str.replace(',', '').replace('BST', '').replace('UTC', '')\n",
    "        date_str = date_str.split(' ')[-3:] if 'am' in date_str.lower() or 'pm' in date_str.lower() else date_str\n",
    "        date_str = ' '.join(date_str).strip() if isinstance(date_str, list) else date_str\n",
    "        # Handle specific cases like '18:29, 24 Aug 2023' by reversing the order to '24 Aug 2023 18:29'\n",
    "        if ':' in date_str.split(' ')[0]:\n",
    "            parts = date_str.split(', ')\n",
    "            if len(parts) > 1:\n",
    "                time_part, date_part = parts\n",
    "                date_str = f\"{date_part} {time_part}\"\n",
    "        \n",
    "        # Try parsing the date\n",
    "        try:\n",
    "            standardized_date = parser.parse(date_str)\n",
    "        except ValueError:\n",
    "            # If parsing fails, handle the specific cases\n",
    "            pass\n",
    "\n",
    "    # Return the formatted date as \"Month Year\" (e.g., \"April 2023\")\n",
    "    return standardized_date.strftime('%B %Y') if pd.notnull(standardized_date) else None\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "def standardize_date(date_str):\n",
    "    standardized_date = pd.NaT  # Defaulting to 'Not a Time' for error handling\n",
    "\n",
    "    # Remove any time or timezone information as we don't care about it\n",
    "    date_str = date_str.split(',')[0].split('BST')[0].strip()\n",
    "    \n",
    "    # Try parsing the date\n",
    "    try:\n",
    "        standardized_date = parser.parse(date_str)\n",
    "    except ValueError:\n",
    "        # If parsing fails, handle the specific cases\n",
    "        pass\n",
    "\n",
    "    return standardized_date.strftime('%B %Y') if pd.notnull(standardized_date) else None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_dates['publish_date'] = df_w_dates['publish_date'].apply(standardize_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_dates.head()\n",
    "# Export the DataFrame df_w_dates to a CSV file\n",
    "df_w_dates.to_csv('df_w_dates.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Network Visualisation\n",
    "\n",
    "## Started by creating the network based on previously processes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "shared_labels = defaultdict(lambda: defaultdict(int))\n",
    "for _, row in df.iterrows():\n",
    "    shared_labels[row['news_source']][row['label']] += 1\n",
    "\n",
    "\n",
    "total_articles = df['news_source'].value_counts()\n",
    "\n",
    "top_three_labels_per_source = df.groupby('news_source')['label'].agg(lambda x: x.value_counts().nlargest(3).index.tolist())\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with size attribute and top three labels as a title\n",
    "for news_source, count in total_articles.items():\n",
    "    G.add_node(news_source, size=count, title=\", \".join(top_three_labels_per_source[news_source]))\n",
    "\n",
    "# determine the connections based on shared labels and find the most common label for each connection\n",
    "connections = defaultdict(list)\n",
    "most_common_label_per_connection = {}\n",
    "for source_a in shared_labels:\n",
    "    for source_b in shared_labels:\n",
    "        if source_a != source_b:\n",
    "            common_labels = set(shared_labels[source_a]) & set(shared_labels[source_b])\n",
    "            weight = sum(min(shared_labels[source_a][label], shared_labels[source_b][label]) for label in common_labels)\n",
    "            if weight > 0:\n",
    "                filtered_common_labels = [label for label in common_labels if shared_labels[source_a][label] > 0.5 and shared_labels[source_b][label] > 0.5]\n",
    "                if filtered_common_labels:\n",
    "                    most_common_label = max(filtered_common_labels, key=lambda label: min(shared_labels[source_a][label], shared_labels[source_b][label]))\n",
    "                    connections[source_a].append((source_b, weight))\n",
    "                    most_common_label_per_connection[(source_a, source_b)] = most_common_label\n",
    "# Sort connections based on weight and limit to top 1-3 connections per node\n",
    "for source, edges in connections.items():\n",
    "    sorted_edges = sorted(edges, key=lambda x: x[1], reverse=True)[:3]  # Keep only top 1-3 connections based on weight\n",
    "    for target, weight in sorted_edges:\n",
    "        G.add_edge(source, target, weight=weight, label=most_common_label_per_connection[(source, target)])\n",
    "\n",
    "# Normalize node sizes for visualization\n",
    "max_size = max(nx.get_node_attributes(G, 'size').values())\n",
    "min_size = 10  # Minimum node size\n",
    "for node in G.nodes:\n",
    "    G.nodes[node]['size'] = min_size + (G.nodes[node]['size'] / max_size) * 50  # Adjust this formula as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the final network visualisation in Pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network_visualization.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"network_visualization.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x14df87b90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt = Network(height=\"750\",notebook=True, cdn_resources='remote', bgcolor=\"#181717\", font_color=\"white\", select_menu=True, filter_menu=True)\n",
    "\n",
    "# Set the physics layout of the network\n",
    "nt.options.physics.enabled = True\n",
    "\n",
    "\n",
    "\n",
    "top_5_nodes = sorted(G.nodes(data=True), key=lambda x: x[1]['size'], reverse=True)[:5]\n",
    "top_5_labels = [node[0] for node in top_5_nodes]\n",
    "for node in G.nodes(data=True):\n",
    "    if node[0] in top_5_labels:\n",
    "        # For top 5 nodes, include the label outside\n",
    "        title = node[1]['title'].split(',')[0] if ',' in node[1]['title'] else node[1]['title']\n",
    "        nt.add_node(node[0], size=node[1]['size'], title=node[1]['title'], label =  f\"{node[0]} - {title}\" ,color='440154', font={'size': 20})\n",
    "    else:\n",
    "        title = node[1]['title'].split(',')[0] if ',' in node[1]['title'] else node[1]['title']\n",
    "        nt.add_node(node[0], size=node[1]['size'], label=node[0] , title=node[1]['title'], color='89D28B')\n",
    "        \n",
    "for edge in G.edges(data=True):\n",
    "    source = edge[0]\n",
    "    target = edge[1]\n",
    "    weight = edge[2]['weight']\n",
    "    nt.add_edge(source, target, value=weight * 0.5, color='8EA8F2', title=edge[2]['label'])\n",
    "nt.title = \"News Source Network Visualization\"\n",
    "\n",
    "\n",
    "nt.set_options('''{\n",
    "  \"nodes\": {\n",
    "    \"borderWidth\": null,\n",
    "    \"borderWidthSelected\": null,\n",
    "    \"opacity\": 0.9,\n",
    "      \"scaling\": {\n",
    "      \"label\": {\n",
    "        \"enabled\": true,\n",
    "        \"min\": 21,\n",
    "        \"max\": 30,\n",
    "        \"maxVisible\": 35,\n",
    "        \"drawThreshold\": 1\n",
    "      }\n",
    "      },\n",
    " \n",
    "    \"size\": null\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"color\": {\n",
    "      \"inherit\": true\n",
    "    }\n",
    "  },\n",
    "  \"interaction\": {\n",
    "    \"hover\": true\n",
    "  }\n",
    "}''')\n",
    "\n",
    "\n",
    "nt.show(\"network_visualization.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
